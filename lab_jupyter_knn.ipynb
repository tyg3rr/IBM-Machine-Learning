{"cells":[{"cell_type":"markdown","id":"561610a1-777c-499e-bf4a-5eea44db51d1","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"0c0d11db-0f31-4f97-9b7c-8face9df121b","metadata":{},"outputs":[],"source":["# **K Nearest Neighbor**\n"]},{"cell_type":"markdown","id":"a0170763-7485-4bed-b675-ed8864e1f3d3","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"76f29d53-ab87-4cb2-95d0-75fafa404df8","metadata":{},"outputs":[],"source":["In this lab, you will learn about and practice the K Nearest Neighbor (KNN) model. KNN is a straightforward but very effective model that can be used for both classification and regression tasks. If the feature space is not very large, KNN can be a high-interpretable model because you can explain and understand how a prediction is made by looking at its nearest neighbors.\n"]},{"cell_type":"markdown","id":"aae46409-5e79-4f2a-b5f1-5c0a49ebcde1","metadata":{},"outputs":[],"source":["We will be using a tumor sample dataset containing lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","id":"2520627b-7b48-482d-b042-83f965b94df9","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"86be96d4-22ab-4dcb-9981-f3d04b7e323d","metadata":{},"outputs":[],"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","id":"53b273f1-3ca7-4860-934d-22aad861db35","metadata":{},"outputs":[],"source":["* Train KNN models with different neighbor hyper-parameters\n","* Evaluate KNN models on classification tasks\n","* Tune the number of neighbors and find the optimized one for a specific task\n"]},{"cell_type":"markdown","id":"56e9a55f-d5e3-4490-a9f1-e0923ba52bc2","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"a1ad8194-1c39-44ac-8542-5676a11aafce","metadata":{},"outputs":[],"source":["First, let's install `seaborn` for visualization tasks and import required libraries for this lab.\n"]},{"cell_type":"code","id":"ae4d601f-b90a-4462-90c5-514d5370e79b","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"."]},{"cell_type":"code","id":"6f65b0ae-7233-4324-8aec-93a487babb74","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"b1bcdde3-4579-45f1-9c2c-2b1b7d39f6c8","metadata":{},"outputs":[],"source":["# Define a random seed to reproduce any random process\nrs = 123"]},{"cell_type":"code","id":"12679cf8-a76e-4dec-bf09-890ae0deaee0","metadata":{},"outputs":[],"source":["# Ignore any deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"markdown","id":"a6c027a6-b443-46b5-9b36-353260942a23","metadata":{},"outputs":[],"source":["## Load and explore the tumor sample dataset\n"]},{"cell_type":"markdown","id":"01a83c2d-2ba1-435f-9b42-6e3e8a4b6742","metadata":{},"outputs":[],"source":["We first load the dataset `tumor.csv` as a Pandas dataframe:\n"]},{"cell_type":"code","id":"d86c6c4a-3e9a-415d-9d0d-f937af7b1049","metadata":{},"outputs":[],"source":["# Read datast in csv format\ndataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\ntumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"4a8b0907-dd93-4295-bb80-5700bb296d41","metadata":{},"outputs":[],"source":["Then, let's quickly take a look at the head of the dataframe.\n"]},{"cell_type":"code","id":"fcc18b65-c755-44f3-b999-c5bacf35847c","metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","id":"3e334614-8b3d-49b9-86ca-dc65bc19423f","metadata":{},"outputs":[],"source":["And, display its columns.\n"]},{"cell_type":"code","id":"29ec9ce6-143d-49d9-8e1b-6f63c705b97a","metadata":{},"outputs":[],"source":["tumor_df.columns"]},{"cell_type":"markdown","id":"77170210-432c-473a-80bf-313b4b385250","metadata":{},"outputs":[],"source":["Each observation in this dataset contains lab test results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) or benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","id":"19327dee-6a51-44e9-81f9-7e9dddb94459","metadata":{},"outputs":[],"source":["Then, let's split the dataset into input `X` and output `y`:\n"]},{"cell_type":"code","id":"32c3f082-f628-40b8-a47f-d8024dbdc2df","metadata":{},"outputs":[],"source":["X = tumor_df.iloc[:, :-1]\ny = tumor_df.iloc[:, -1:]"]},{"cell_type":"markdown","id":"a3f8f58f-7030-46aa-b4fb-f71dbb9ac734","metadata":{},"outputs":[],"source":["And, we first check the statistics summary of features in `X`:\n"]},{"cell_type":"code","id":"e09918a5-9351-4e65-9b53-f3f7adbe93a6","metadata":{},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","id":"bb2b41c6-3dc7-4bd5-a31a-14ee21afb98a","metadata":{},"outputs":[],"source":["As we can see from the above cell output, all features are numeric and ranged between 1 to 10. This is very convenient as we do not need to scale the feature values as they are already in the same range.\n"]},{"cell_type":"markdown","id":"a5122b56-7480-479c-9677-dd94977e838a","metadata":{},"outputs":[],"source":["Next, let's check the class distribution of output `y`:\n"]},{"cell_type":"code","id":"e475a5c0-e4c2-4dd4-8a06-6f6d1ae1b868","metadata":{},"outputs":[],"source":["y.value_counts(normalize=True)"]},{"cell_type":"code","id":"bdadf244-ceb0-44b2-a86f-cb692425ad39","metadata":{},"outputs":[],"source":["y.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","id":"5c3dc0b2-b67c-44b0-a121-50219a241bfd","metadata":{},"outputs":[],"source":["We have about 65% benign tumors (`Class = 0`) and 35% cancerous tumors (`Class = 1`), which is not a very imbalanced class distribution. \n"]},{"cell_type":"markdown","id":"85881830-d71d-4b29-81a2-8ed977772fa3","metadata":{},"outputs":[],"source":["## Process and split training and testing datasets\n"]},{"cell_type":"code","id":"9b6f6166-d306-4890-856f-2c29d0b9d6b0","metadata":{},"outputs":[],"source":["# Split 80% as training dataset\n# and 20% as testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"fd61951a-025f-4fc0-9c1c-cead7aa9c2da","metadata":{},"outputs":[],"source":["## Train and evaluate a KNN classifier with the number of neighbors set to 2\n"]},{"cell_type":"markdown","id":"5ed177c6-fa8b-44cb-8a54-56bc83b43b3a","metadata":{},"outputs":[],"source":["Training a KNN classifier is very similar to training other classifiers in `sklearn`, we first need to define a `KNeighborsClassifier` object. Here we use `n_neighbors=2` argument to specify how many neighbors will be used for prediction, and we keep other arguments to be their default values.\n"]},{"cell_type":"code","id":"5d646dc9-c021-4423-89fa-962fd719c9eb","metadata":{},"outputs":[],"source":["# Define a KNN classifier with `n_neighbors=2`\nknn_model = KNeighborsClassifier(n_neighbors=2)"]},{"cell_type":"markdown","id":"900a9a10-a96a-4635-bb69-58be1f75632f","metadata":{},"outputs":[],"source":["Then we can train the model with `X_train` and `y_train`, and we use ravel() method to convert the data frame `y_train` to a vector.\n"]},{"cell_type":"code","id":"0d805c9b-6ecd-47b8-b6a3-1c59d88aab3d","metadata":{},"outputs":[],"source":["knn_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"7e7c2572-3c61-4321-b62e-93e564cfd2fb","metadata":{},"outputs":[],"source":["And, we can make predictions on the `X_test` dataframe.\n"]},{"cell_type":"code","id":"bcf8e940-4e08-40b0-8b79-60a05e3319f8","metadata":{},"outputs":[],"source":["preds = knn_model.predict(X_test)"]},{"cell_type":"markdown","id":"b428f49c-204c-49d1-8eec-f0ac601c5b7e","metadata":{},"outputs":[],"source":["To evaluate the KNN classifier, we provide a pre-defined method to return the commonly used evaluation metrics such as accuracy, recall, precision, f1score, and so on, based on the true classes in the 'y_test' and model predictions.\n"]},{"cell_type":"code","id":"46eb2059-a957-4645-860b-0b29b0315042","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos"]},{"cell_type":"code","id":"1de89478-cf06-4eb0-88d7-85ca045f1686","metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"61ff6f2f-b255-4902-9ed6-8467fe936138","metadata":{},"outputs":[],"source":["We can see that there is a great classification performance on the tumor sample dataset. This means the KNN model can effectively recognize cancerous tumors.\n","Next, it's your turn to try a different number of neighbors to see if we could get even better performance.\n"]},{"cell_type":"markdown","id":"edf82a74-d8c9-46f0-b088-857d07169e25","metadata":{},"outputs":[],"source":["## Coding exercise: Train and evaluate a KNN classifier with number of neighbors set to 5\n"]},{"cell_type":"markdown","id":"c6c3d605-a2e4-4cf2-aeac-28e9125ca524","metadata":{},"outputs":[],"source":["First, define a KNN classifier with KNeighborsClassifier class:\n"]},{"cell_type":"code","id":"65ade8b0-a8a1-4702-bb7e-0a7203a0b709","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"287e3b58-260f-400e-8261-e5ed1fd7b5fe","metadata":{},"outputs":[],"source":["Then train the model with `X_train` and `y_train`:\n"]},{"cell_type":"code","id":"95c17752-2f86-4791-b2ba-7bc52d249252","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"10483d96-32f0-4cf9-878e-0722daf3d2c0","metadata":{},"outputs":[],"source":["And, make predictions on `X_test` dataframe:\n"]},{"cell_type":"code","id":"d4866502-3fca-4e3d-9ced-48664f66f698","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"2cf200be-2cdb-4e92-80f1-bb3c04e15f4f","metadata":{},"outputs":[],"source":["At last, you can evaluate your KNN model with provided `evaluate_metrics()` method.\n"]},{"cell_type":"markdown","id":"17b8682e-a3f4-4f8f-9554-acbd1d1626d0","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for a sample solution\u003c/summary\u003e\n","\n","```python\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"90f022f9-14de-4155-a9fb-2b15d845b341","metadata":{},"outputs":[],"source":["## Tune the number of neighbors to find the optmized one\n"]},{"cell_type":"markdown","id":"9e5825c9-96a3-4dd9-b673-a74c8009021e","metadata":{},"outputs":[],"source":["OK, you may wonder which `n_neighbors` argument may give you the best classification performance. We can try different `n_neighbors` (the K value) and check which `K` gives the best classification performance.\n"]},{"cell_type":"markdown","id":"38f46daa-e620-4256-ab6c-fba60d7e2b0a","metadata":{},"outputs":[],"source":["Here we could try K from 1 to 50, and store the aggregated `f1score` for each k into a list.\n"]},{"cell_type":"code","id":"220a3acf-4da1-48fc-8db4-3229c27d6bd0","metadata":{},"outputs":[],"source":["# Try K from 1 to 50\nmax_k = 50\n# Create an empty list to store f1score for each k\nf1_scores = []"]},{"cell_type":"markdown","id":"8e0cab76-254e-48f5-b52e-854c753b3e45","metadata":{},"outputs":[],"source":["Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n"]},{"cell_type":"code","id":"a9f621a0-5dd1-47c3-ad4c-c76bb2eef537","metadata":{},"outputs":[],"source":["for k in range(1, max_k + 1):\n    # Create a KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Train the classifier\n    knn = knn.fit(X_train, y_train.values.ravel())\n    preds = knn.predict(X_test)\n    # Evaluate the classifier with f1score\n    f1 = f1_score(preds, y_test)\n    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n# Convert the f1score list to a dataframe\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nf1_results.set_index('K')"]},{"cell_type":"markdown","id":"e8cf6e18-f8b2-492d-af5c-9b64b5cd2da2","metadata":{},"outputs":[],"source":["This is a long list and different to analysis, so let's visualize the list using a linechart.\n"]},{"cell_type":"code","id":"c21ecc80-f799-4ec2-a319-f4711e08b0c1","metadata":{},"outputs":[],"source":["# Plot F1 results\nax = f1_results.plot(figsize=(12, 12))\nax.set(xlabel='Num of Neighbors', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.ylim((0.85, 1))\nplt.title('KNN F1 Score')"]},{"cell_type":"markdown","id":"523a7f47-57b0-4fc1-bd0d-6d186f0468ca","metadata":{},"outputs":[],"source":["As we can see from the F1 score linechart, the best `K` value is 5 with about `0.9691` f1score.\n"]},{"cell_type":"markdown","id":"051f1e75-a5fd-4aae-aeed-5de0f58c7d08","metadata":{},"outputs":[],"source":["## Next steps\n"]},{"cell_type":"markdown","id":"242d4be7-f067-4788-ab81-a1577e236a5b","metadata":{},"outputs":[],"source":["Great! Now you have learned about and applied the KNN model to solve a real-world tumor type classification problem. You also tuned the KNN to find the best K value. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"985182af-35cc-4378-9677-8f059a2180df","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"ed2beb39-58b8-4e28-89d2-7dff298369be","metadata":{},"outputs":[],"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"]},{"cell_type":"markdown","id":"8576ebc8-63f0-48a2-9ad0-4f93d3210564","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"5d88f48a-e243-4573-bda7-2294fe721378","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"b3c6c179-0b84-402b-86a7-d993f132c012","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-11-9|1.0|Yan|Created the initial version|\n","|2022-3-29|1.1|Steve Hord|QA Pass|\n"]},{"cell_type":"markdown","id":"ca04275e-b184-40e8-8a16-995d1bf302b6","metadata":{},"outputs":[],"source":["Copyright Â© 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}